{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to load datasets form sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "iris = datasets.load_iris()\n",
    "diabetes  = datasets.load_diabetes()\n",
    "wine = datasets.load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "y  = pd.DataFrame(iris.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to apply standardization (also known as z-score normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to split the data and train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to build a classfication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n",
    "from sklearn.metrics import classification_report, precision_score, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find all metrics to evaluate a classfication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = classification_report(y_test, y_pred)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = pd.DataFrame(data= diabetes.data, columns=diabetes.feature_names)\n",
    "dfd[\"target\"] = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfd.drop(\"target\", axis = 1)\n",
    "y = dfd[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to apply grid search to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'n_jobs': [1,2]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering, Non - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans()\n",
    "\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Labels:\", labels)\n",
    "print(\"Centroids:\", centroids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative (bottom-up) - Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels = clustering.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Labels:\", labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "transactions = [['bread', 'milk', 'eggs'],\n",
    "                ['bread', 'coffee'],\n",
    "                ['bread', 'milk', 'coffee', 'eggs'],\n",
    "                ['bread', 'milk', 'coffee'],\n",
    "                ['bread', 'milk'],\n",
    "                ['bread', 'milk', 'coffee'],\n",
    "                ['bread'],\n",
    "                ['bread', 'milk', 'coffee', 'eggs']]\n",
    "\n",
    "# Transform the transaction data into a binary encoded format\n",
    "encoder = TransactionEncoder()\n",
    "transaction_array = encoder.fit_transform(transactions)\n",
    "df = pd.DataFrame(transaction_array, columns=encoder.columns_)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(association_rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Yellow']})\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['Color']])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Color']))\n",
    "\n",
    "final_df = pd.concat([data, encoded_df], axis=1)\n",
    "print(final_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Yellow']})\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_data = encoder.fit_transform(data['Color'])\n",
    "encoded_data = pd.DataFrame(data=encoded_data, columns=['Encoded_Color'])\n",
    "\n",
    "df = pd.concat([data, encoded_data], axis=1)\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "sample1 = [1, 2, 3, 4, 5]\n",
    "sample2 = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Perform independent t-test\n",
    "t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
    "\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value*100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to apply bagging ensemble technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create individual base classifiers\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = SVC(kernel='linear')\n",
    "classifier3 = SVC(kernel='rbf')\n",
    "\n",
    "# Create a list of base classifiers\n",
    "base_classifiers = [('dt', classifier1), ('svc_linear', classifier2), ('svc_rbf', classifier3)]\n",
    "\n",
    "# Create a Voting classifier from the base classifiers\n",
    "voting_classifier = VotingClassifier(estimators=base_classifiers)\n",
    "\n",
    "# Create a Bagging ensemble of Voting classifiers\n",
    "bagging_classifier = BaggingClassifier(voting_classifier, n_estimators=10)\n",
    "\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = bagging_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to apply boosting ensemble technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (example: Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base classifier (example: Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Create an AdaBoost ensemble classifier with the base classifier\n",
    "boosting_classifier = AdaBoostClassifier(base_classifier, n_estimators=10)\n",
    "\n",
    "boosting_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = boosting_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to apply Stacking ensemble technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the base classifiers\n",
    "base_classifiers = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "# Create the meta-classifier\n",
    "meta_classifier = LogisticRegression()\n",
    "\n",
    "# Create the stacking ensemble classifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)\n",
    "\n",
    "stacking_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = stacking_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to perform Winsorization to treat outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "   col1  col2\n",
      "0    10     5\n",
      "1    20    15\n",
      "2    25    20\n",
      "3    30    25\n",
      "4   100    30\n",
      "5   200   150\n",
      "6   250   250\n",
      "7   300   500\n",
      "\n",
      "Winsorized data:\n",
      "   col1     col2\n",
      "0    10    5.000\n",
      "1    20   15.000\n",
      "2    25   20.000\n",
      "3    30   25.000\n",
      "4   100   30.000\n",
      "5   200  150.000\n",
      "6   250  250.000\n",
      "7   300  409.375\n"
     ]
    }
   ],
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'col1': [10, 20, 25, 30, 100, 200, 250, 300],\n",
    "    'col2': [5, 15, 20, 25, 30, 150, 250, 500]\n",
    "})\n",
    "\n",
    "# Apply winsorization to the dataset using IQR\n",
    "winsorizer = Winsorizer(capping_method='iqr', tail='both', fold=1.5)\n",
    "winsorized_data = winsorizer.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nWinsorized data:\")\n",
    "print(winsorized_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_14th-march-2022-0NJQoJ6G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
