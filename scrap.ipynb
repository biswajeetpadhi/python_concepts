{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send HTTP request to the target URL\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Retrieve the HTML content\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract the desired data using selectors or XPath\n",
    "title = soup.find(\"h1\").text\n",
    "paragraphs = [p.text for p in soup.find_all(\"p\")]\n",
    "\n",
    "# Store the data in a desired format\n",
    "data = {\n",
    "    \"title\": title,\n",
    "    \"paragraphs\": paragraphs\n",
    "}\n",
    "\n",
    "# Example: Print the scraped data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types of html parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"<html><body><h1>Hello, BeautifulSoup!</h1></body></html>\"\n",
    "\n",
    "# Using the \"html.parser\" parser\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Using the \"lxml\" parser\n",
    "soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "# Using the \"html5lib\" parser\n",
    "soup = BeautifulSoup(html_content, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using try-except blocks for error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # Process the response or extract data here\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Handle request exceptions, such as connection errors or invalid URLs\n",
    "    print(\"Request error:\", e)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    # Handle HTTP errors, such as 404 or 500 status codes\n",
    "    print(\"HTTP error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # Process the response or extract data here\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Log the error using the logging module\n",
    "    logging.error(\"Request error: %s\", e)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    logging.error(\"HTTP error: %s\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrying failed requests with exponential backoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "\n",
    "for _ in range(MAX_RETRIES):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        # Process the response or extract data here\n",
    "        break  # Break the loop if the request is successful\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request error:\", e)\n",
    "        # Wait for a delay before retrying\n",
    "        time.sleep(RETRY_DELAY)\n",
    "else:\n",
    "    print(\"Max retries exceeded, request failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing timeouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    # Process the response or extract data here\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Request error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to simulate user authentication using the requests library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login failed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the login page\n",
    "login_url = 'https://example.com/login'\n",
    "\n",
    "# User credentials\n",
    "username = 'your_username'\n",
    "password = 'your_password'\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Send a GET request to the login page to retrieve necessary cookies and tokens\n",
    "response = session.get(login_url)\n",
    "\n",
    "# Extract necessary form data (such as CSRF token) from the response\n",
    "# ...\n",
    "\n",
    "# Prepare the login payload with the required form data\n",
    "payload = {\n",
    "    'username': username,\n",
    "    'password': password,\n",
    "    'csrf_token': 'your_csrf_token'\n",
    "}\n",
    "\n",
    "# Send a POST request to the login page with the login payload\n",
    "response = session.post(login_url, data=payload)\n",
    "\n",
    "# Check if authentication was successful (based on the response)\n",
    "if response.status_code == 200:\n",
    "    print(\"Login successful\")\n",
    "else:\n",
    "    print(\"Login failed\")\n",
    "\n",
    "# Now, you can use the authenticated session to access other protected pages\n",
    "response = session.get('https://example.com/protected_page')\n",
    "\n",
    "# Process the response and extract the desired data\n",
    "# ...\n",
    "\n",
    "# Close the session (optional)\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ethical considerations while web scraping using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website does not have a robots.txt file\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# URL of the website you intend to scrape\n",
    "url = 'https://example.com/'\n",
    "\n",
    "# Send a GET request to the website's robots.txt file\n",
    "robots_url = urlparse(url)._replace(path='/robots.txt').geturl()\n",
    "response = requests.get(robots_url)\n",
    "\n",
    "# Check if the website has a robots.txt file\n",
    "if response.status_code == 200:\n",
    "    robots_txt = response.text\n",
    "    # Parse the robots.txt file and extract the rules\n",
    "    # ...\n",
    "\n",
    "    # Check if the website allows scraping\n",
    "    if 'User-agent: *' in robots_txt and 'Disallow: /' not in robots_txt:\n",
    "        # Respect rate limits and avoid overloading the website's servers\n",
    "        delay_between_requests = 2  # Set an appropriate delay between requests\n",
    "        headers = {'User-Agent': 'Your User Agent'}  # Set a valid User-Agent header\n",
    "\n",
    "        # Start scraping the website\n",
    "        while True:\n",
    "            try:\n",
    "                # Send a GET request to the website\n",
    "                response = requests.get(url, headers=headers)\n",
    "                # Process the response or extract the desired data\n",
    "                # ...\n",
    "\n",
    "                # Respect rate limits and avoid overloading the website's servers\n",
    "                time.sleep(delay_between_requests)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Handle request exceptions, such as connection errors\n",
    "                print(\"Request error:\", e)\n",
    "\n",
    "            # Break the loop if scraping is complete or a condition is met\n",
    "            # ...\n",
    "\n",
    "        # Ensure the scraped data is used responsibly and legally\n",
    "        # ...\n",
    "\n",
    "    else:\n",
    "        print(\"Scraping not allowed by the website's robots.txt file\")\n",
    "else:\n",
    "    print(\"Website does not have a robots.txt file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement monitoring and maintenance features in a Python web scraping script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Target URL to scrape\n",
    "url = 'https://example.com/'\n",
    "\n",
    "# Function to scrape the website\n",
    "def scrape_website():\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Process the response or extract the desired data\n",
    "            # ...\n",
    "            \n",
    "            # Log a success message\n",
    "            logging.info('Scraping successful')\n",
    "        else:\n",
    "            # Log an error message\n",
    "            logging.error('Request failed with status code: ' + str(response.status_code))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log the exception message\n",
    "        logging.error('Request error: ' + str(e))\n",
    "        # Raise the exception to be handled at a higher level\n",
    "        raise\n",
    "\n",
    "# Function to periodically scrape the website\n",
    "def run_scraping_job():\n",
    "    while True:\n",
    "        try:\n",
    "            # Scrape the website\n",
    "            scrape_website()\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the exception message\n",
    "            logging.error('An error occurred: ' + str(e))\n",
    "            # Implement additional error handling or notifications if needed\n",
    "            \n",
    "        # Wait for a specified interval before scraping again\n",
    "        time.sleep(3600)  # Scrape every hour\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Run the scraping job\n",
    "        run_scraping_job()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        # Log a message when the script is interrupted by the user\n",
    "        logging.info('Script interrupted by user')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement proxies or rotating IP addresses in Python using the Requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request error: HTTPSConnectionPool(host='example.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002E5E05D2050>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://example.com/'\n",
    "\n",
    "# List of proxies\n",
    "proxies = [\n",
    "    'http://proxy1.example.com:8080',\n",
    "    'http://proxy2.example.com:8080',\n",
    "    'http://proxy3.example.com:8080',\n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Function to scrape the website using a proxy\n",
    "def scrape_website_with_proxy():\n",
    "    try:\n",
    "        # Select a proxy from the list\n",
    "        proxy = proxies.pop(0)\n",
    "        \n",
    "        # Send a GET request to the website using the proxy\n",
    "        response = requests.get(url, proxies={'http': proxy, 'https': proxy})\n",
    "        \n",
    "        # Process the response or extract the desired data\n",
    "        # ...\n",
    "        \n",
    "        # Add the used proxy back to the list for rotation\n",
    "        proxies.append(proxy)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle request exceptions\n",
    "        print('Request error:', e)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    # Scrape the website using a proxy\n",
    "    scrape_website_with_proxy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject-NzVrMXGb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
